import numpy as np
import json
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sentence_transformers import SentenceTransformer
import hdbscan
from sklearn.metrics.pairwise import cosine_similarity
# ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°©ì‹ì¸ ModelInference ì‚¬ìš©
from ibm_watsonx_ai.foundation_models import ModelInference

from database import SessionLocal, engine
from models import Base, Article, Issue

# -------------------------------------------------
# 1. ëª¨ë¸ ë¡œë“œ (Embedding & LLM)
# -------------------------------------------------
print("--- [AI] ëª¨ë¸ ë¡œë”© ì¤‘... ---")
embed_model = SentenceTransformer("BAAI/bge-m3")

# Watsonx Llama-3-3-70b ì„¤ì • (llmtest.pyì—ì„œ ì„±ê³µí•œ ì„¤ì • ë°˜ì˜)
credentials = {
    "apikey":  # ì‚¬ìš©ìžë‹˜ì˜ í‚¤ ìœ ì§€
    "url": "https://us-south.ml.cloud.ibm.com"
}

llm_model = ModelInference(
    model_id="meta-llama/llama-3-3-70b-instruct",
    credentials=credentials,
    project_id=
)

# -------------------------------------------------
# 2. Stage 2: LLM ê²€ì¦ ë° ìš”ì•½ í•¨ìˆ˜
# -------------------------------------------------
def run_stage2_issue_refine(cluster_articles):
    titles = [a.title for a in cluster_articles]
    
    # [ë¡œê·¸] í˜„ìž¬ ê²€ì¦ ëŒ€ìƒ ì¶œë ¥
    print(f"\n      [LLM ê²€ì¦ ì¤‘] ê¸°ì‚¬ {len(titles)}ê±´ í›„ë³´ ë°œê²¬")
    for i, t in enumerate(titles[:3]): # ìµœëŒ€ 3ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
        print(f"        - {t}")
    if len(titles) > 3: print(f"        ... ì™¸ {len(titles)-3}ê±´")

    # í”„ë¡¬í”„íŠ¸ ë³´ê°•: íŒ©íŠ¸ ì²´í¬ ë° ì¸ë¬¼ ê´€ê³„ ëª…ì‹œ ìš”ì²­
    prompt = f"""
ì—­í• : ë„ˆëŠ” ë‰´ìŠ¤ íŽ¸ì§‘ìžë‹¤. ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ë“¤ì´ 'í•˜ë‚˜ì˜ ë™ì¼í•œ êµ¬ì²´ì  ì‚¬ê±´'ì¸ì§€ íŒë‹¨í•˜ë¼.
ë‹¨ìˆœížˆ ì¹´í…Œê³ ë¦¬ê°€ ê°™ì€ ê²ƒì€ í•˜ë‚˜ì˜ ì´ìŠˆê°€ ì•„ë‹ˆë‹¤.

[ë‰´ìŠ¤ ì œëª© ëª©ë¡]
{chr(10).join(f"- {t}" for t in titles)}

ìš”ì²­:
1. ì´ ë¬¶ìŒì´ ë™ì¼í•œ ì‚¬ê±´ì„ ë‹¤ë£¨ëŠ”ì§€ íŒë‹¨í•˜ë¼ (is_issue: True/False)
2. íŒë‹¨ ì´ìœ ë¥¼ ì ì–´ë¼ (reason: ) - ì˜ˆ: ì£¼ì²´ì™€ ì‚¬ê±´ì˜ ë‚´ìš©ì´ ì¼ì¹˜í•¨ / ì„œë¡œ ë‹¤ë¥¸ ì‚¬ê±´ìž„
3. ë™ì¼ ì‚¬ê±´ì´ë¼ë©´ ë…ìžê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ëŒ€í‘œ ì œëª©ì„ ìƒì„±í•˜ë¼ (title: )
4. ì‚¬ê±´ì˜ í•µì‹¬ì„ 2ë¬¸ìž¥ ì´ë‚´ë¡œ ìš”ì•½í•˜ë¼ (summary: ) - ì¸ë¬¼ ê´€ê³„(ì˜ˆ: êµì‚¬-êµì‚¬)ë¥¼ ì •í™•ížˆ ëª…ì‹œí•  ê²ƒ.

ì¶œë ¥ í˜•ì‹:
is_issue: 
reason:
title: 
summary: 
"""
    try:
        # llmtest.pyì—ì„œ ì„±ê³µí•œ íŒŒë¼ë¯¸í„° ì ìš©
        response = llm_model.generate_text(
            prompt=prompt,
            params={
                "max_new_tokens": 500,
                "temperature": 0.1, # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
                "decoding_method": "sample"
            }
        )
        
        # íŒŒì‹± ë¡œì§
        res_dict = {}
        lines = response.strip().split('\n')
        for line in lines:
            if ':' in line:
                key, val = line.split(':', 1)
                res_dict[key.strip().lower()] = val.strip()
        
        # [ë¡œê·¸] LLMì˜ íŒë‹¨ ê²°ê³¼ ì¶œë ¥
        status_icon = "âœ…" if res_dict.get('is_issue') == 'True' else "âŒ"
        print(f"      {status_icon} ê²°ê³¼: {res_dict.get('is_issue')}")
        print(f"      ðŸ“ ì´ìœ : {res_dict.get('reason')}")
        if res_dict.get('is_issue') == 'True':
            print(f"      ðŸ’¡ ìƒì„±ëœ ì œëª©: {res_dict.get('title')}")
            
        return res_dict
    except Exception as e:
        print(f"      âš ï¸ [LLM Error] {e}")
        return None

# -------------------------------------------------
# 3. ë©”ì¸ í´ëŸ¬ìŠ¤í„°ë§ í•¨ìˆ˜
# -------------------------------------------------
def run_issue_clustering(db: Session, days: int = 1):
    Base.metadata.create_all(bind=engine)

    # ë¯¸ë¶„ë¥˜ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
    time_threshold = datetime.now() - timedelta(days=days)
    articles = db.query(Article).filter(
        Article.time >= time_threshold,
        Article.issue_id.is_(None)
    ).all()

    if len(articles) < 3:
        print(f"--- [Skip] ë¶„ë¥˜í•  ê¸°ì‚¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ({len(articles)}ê°œ) ---")
        return

    # [Stage 1] ìž„ë² ë”© ë° HDBSCAN
    print(f"--- [Stage 1] {len(articles)}ê°œ ê¸°ì‚¬ ìž„ë² ë”© ìƒì„± ì¤‘... ---")
    # ì œëª© ë°˜ë³µì„ í†µí•´ ê³ ìœ ëª…ì‚¬ ê°€ì¤‘ì¹˜ ê°•í™”
    input_texts = [f"{a.title} {a.title} {(a.contents or '')[:50]}" for a in articles]
    embeddings = embed_model.encode(input_texts, normalize_embeddings=True)

    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=2,
        min_samples=1,
        metric="euclidean",
        cluster_selection_epsilon=0.35,
        cluster_selection_method="eom"
    )
    labels = clusterer.fit_predict(embeddings)

    unique_clusters = set(labels)
    print(f"--- [Stage 2] í›„ë³´ êµ°ì§‘ ê²€ì¦ ì‹œìž‘ ---")

    for cluster_id in unique_clusters:
        if cluster_id == -1: continue

        indices = np.where(labels == cluster_id)[0]
        cluster_articles = [articles[i] for i in indices]

        # [Stage 2] LLMì—ê²Œ ìµœì¢… í™•ì¸ ë° ìš”ì•½ ìš”ì²­
        refine_result = run_stage2_issue_refine(cluster_articles)

        if not refine_result or refine_result.get('is_issue') != 'True':
            print(f"   [Skip] LLMì´ ì´ìŠˆê°€ ì•„ë‹ˆë¼ê³  íŒë‹¨í•¨ (ê¸°ì‚¬ {len(cluster_articles)}ê±´)")
            continue

        # [Step 6] ìµœì¢… DB ì €ìž¥
        try:
            new_issue = Issue(
                title=refine_result.get('title', cluster_articles[0].title),
                contents=refine_result.get('summary', "ìš”ì•½ ì •ë³´ ì—†ìŒ"),
                analysis_result={
                    "status": "verified",
                    "article_count": len(cluster_articles),
                    "reason": refine_result.get('reason')
                },
                created_at=datetime.now()
            )
            db.add(new_issue)
            db.flush()

            for a in cluster_articles:
                a.issue_id = new_issue.id

            print(f"   âœ… ì´ìŠˆ í™•ì •: {new_issue.title}")

        except Exception as e:
            db.rollback()
            print(f"   âŒ ì €ìž¥ ì‹¤íŒ¨: {e}")

    db.commit()
    print("--- [Success] í´ëŸ¬ìŠ¤í„°ë§ ë° ê²€ì¦ ì™„ë£Œ ---")

if __name__ == "__main__":
    db = SessionLocal()
    try:
        run_issue_clustering(db, days=3)
    finally:
        db.close()