import requests
import time
import random
from bs4 import BeautifulSoup


headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

# --- [íŒŒíŠ¸ 1] ì‚¬ìš©ìë‹˜ì´ ë§Œë“œì‹  ê³ ì„±ëŠ¥ ìƒì„¸ ìˆ˜ì§‘ê¸° (ê·¸ëŒ€ë¡œ ì‚¬ìš©) ---
def get_news_details(url):

    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')

        # 1. ì œëª©
        title = soup.select_one('h2#title_area span').get_text(strip=True) if soup.select_one('h2#title_area span') else "ì œëª© ì—†ìŒ"

        # 2. ì‹œê°„
        time_el = soup.select_one('.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME')
        published_at = time_el['data-date-time'] if time_el and time_el.has_attr('data-date-time') else None

        # 3. ì‹ ë¬¸ì‚¬
        publisher = soup.select_one('.media_end_head_top_logo img')['title'] if soup.select_one('.media_end_head_top_logo img') else "ì–¸ë¡ ì‚¬ ë¯¸ìƒ"

        # 4. ì´ë¯¸ì§€ (ëŒ€í‘œ ì´ë¯¸ì§€ 1ê°œë§Œ DBì— ë„£ê¸° ìœ„í•´ ì²« ë²ˆì§¸ ê²ƒë§Œ ê°€ì ¸ì˜´)
        image_url = None
        img_tag = soup.select_one('#newsct_article img')
        if img_tag:
            image_url = img_tag.get('data-src') or img_tag.get('src')

        # 5. ë³¸ë¬¸ ë‚´ìš© (ë…¸ì´ì¦ˆ ì œê±° ë¡œì§ ìœ ì§€)
        content_area = soup.select_one('#newsct_article')
        if content_area:
            for extra in content_area.select('.img_desc, .article_caption, em, script, style'):
                extra.decompose()
            content = content_area.get_text(separator=' ', strip=True)
        else:
            content = "ë‚´ìš© ì—†ìŒ"

        return {
            "title": title,
            "published_at": published_at,
            "publisher": publisher,
            "content": content,
            "image_url": image_url,
            "url": url
        }

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
        return None


# --- [íŒŒíŠ¸ 2] ê²€ìƒ‰í•´ì„œ URLì„ ë¬¼ì–´ì˜¤ëŠ” íƒìƒ‰ê¸° (ìƒˆë¡œ ì¶”ê°€ë¨) ---
def crawl_breaking_news(limit=20, db_check_session=None):
    """
    ë„¤ì´ë²„ ì†ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    db_check_sessionì´ ìˆìœ¼ë©´ ì¤‘ë³µ ê¸°ì‚¬ í™•ì¸ ì‹œ ìˆ˜ì§‘ì„ ë©ˆì¶¥ë‹ˆë‹¤.
    """
    # 1. í•¨ìˆ˜ ì•ˆì—ì„œ crud í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
    from crud import is_url_exists 

    print(f"ğŸ“¡ ì†ë³´ í™•ì¸ ì¤‘... (ìµœëŒ€ {limit}ê°œ íƒìƒ‰)")
    
    # 2. ì†ë³´ í˜ì´ì§€ ìš”ì²­
    base_url = "https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=001"
    response = requests.get(base_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    articles_ul = soup.select('.list_body ul.type06_headline li, .list_body ul.type06 li')
    
    results = []
    collected_urls = set()
    count = 0

    for li in articles_ul:
        if count >= limit:
            break
            
        a_tags = li.select('dl dt a')
        for a in a_tags:
            url = a['href']
            
            # ì¤‘ë³µ URLì´ê±°ë‚˜ ë„¤ì´ë²„ ë‰´ìŠ¤ê°€ ì•„ë‹ˆë©´ íŒ¨ìŠ¤
            if url in collected_urls or "news.naver.com" not in url:
                continue
            
            # ğŸ”¥ [í•µì‹¬] DBì— ì´ë¯¸ ìˆëŠ” ê¸°ì‚¬ì¸ì§€ í™•ì¸
            if db_check_session and is_url_exists(db_check_session, url):
                print(f"  ğŸ›‘ [ì¤‘ë‹¨] ì´ë¯¸ ìˆ˜ì§‘í•œ ê¸°ì‚¬ë¥¼ ë§Œë‚¬ìŠµë‹ˆë‹¤! ({url})")
                print(f"     ì´ ì´í›„ë¡œëŠ” ì´ì „ ë‰´ìŠ¤ì´ë¯€ë¡œ ìˆ˜ì§‘ì„ ë©ˆì¶¥ë‹ˆë‹¤.")
                return results # ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ

            collected_urls.add(url)
            
            # ìƒì„¸ ìˆ˜ì§‘
            print(f"  [{count+1}] ìƒˆ ë‰´ìŠ¤ ìˆ˜ì§‘: {url}")
            article = get_news_details(url) # ìœ„ì— ì •ì˜ëœ ìƒì„¸ ìˆ˜ì§‘ í•¨ìˆ˜ í˜¸ì¶œ
            
            if article:
                results.append(article)
                count += 1
                if count >= limit:
                    break
            
            # ì°¨ë‹¨ ë°©ì§€ìš© ë”œë ˆì´
            time.sleep(random.uniform(0.3, 0.8))

    return results